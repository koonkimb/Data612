{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0H9ypPoh8yfE"
   },
   "source": [
    "# Final Project Planning Document\n",
    "\n",
    "For my final project, I would like to use my Project 4 dataset which contained user preference data from 73,516 users and 12,294 anime.  In Project 4, I was unable to perform certain operations in Pandas with the full dataset due to memory constraints on my computer.  In Project 5, I was unable to compute cosine-similarity for a smaller dataset as my local Spark session would crash.  As a result, I would like to learn to overcome both of these obstacles through leveraging AWS alongside more optimized code.  In Project 4, my content based recommender system used the genre column to determine similarity between anime.  Since the data was sourced from myanimelist.net, which does contain anime descriptions, I will additionally scrape the description data and use that for the TF-IDF and cosine similarity analysis.  In this version of my project, there are also certain genre of anime that I will exclude, as I would like to build a family-friendly recommender system.  \n",
    "\n",
    "To scrape the data, I will use BeautifulSoup in Python and parse data from the description html tags.  The id in the dataset will match the anime_id in the original dataset. \n",
    "\n",
    "My plan is to use Amazon S3 for data storage and create the recommender system in Amazon EC2.  This can be done with the free trial version of AWS.  I will use Sci-kit Learn for the TF-IDF analysis.  Potentially, I will use Amazon Sagemaker as an alternative to EC2 as it can offer more memory and also has built in Spark containers.  However, this may incur additional costs beyond the free tier.\n",
    "\n",
    "As suggested by another classmate in our discussions board, my strategy will be to develop the code locally with PySpark on a smaller subset of the data, and then bring it over to AWS once I have a working prototype.  Spark set-up and some preliminary pre-processing steps are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK7TEq9oAJea"
   },
   "source": [
    "# Spark Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-K2VAS2U:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1aa2ed8aba0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"C:\\\\Temp\\\\spark-temp\"\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"6g\").config(\"spark.executor.memory\", \"6g\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-4eBUovHMLup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\.cache\\kagglehub\\datasets\\CooperUnion\\anime-recommendations-database\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import gc\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat_ws, col, lit, mean, stddev, stddev_samp, count_distinct, when, split\n",
    "# Authenticate\n",
    "# kagglehub.login()\n",
    "path = kagglehub.dataset_download(\"CooperUnion/anime-recommendations-database\")\n",
    "print(path)\n",
    "path_anime = path + '\\\\anime.csv'\n",
    "path_rating = path + '\\\\rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anime = pd.read_csv(path_anime, header = 0)\n",
    "anime = spark.read.csv(path_anime, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anime_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- episodes: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- members: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anime_id', 'name', 'genre', 'type', 'episodes', 'rating', 'members']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType,StructField,StructType, BooleanType, DoubleType\n",
    "anime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('anime_id', IntegerType()), \n",
    "    StructField('name', StringType()),\n",
    "    StructField('genre', StringType()),\n",
    "    StructField('type', StringType()),\n",
    "    StructField('episodes', DoubleType()),\n",
    "    StructField('rating', DoubleType()),\n",
    "    StructField('members', IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = spark.read.csv(path_anime, schema = schema, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anime_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- episodes: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- members: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- anime_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rating = pd.read_csv(path_ratings, header = 0)\n",
    "rating = spark.read.csv(path_rating, header = True)\n",
    "rating.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_schema = StructType([\n",
    "    StructField('user_id', IntegerType()), \n",
    "    StructField('anime_id', IntegerType()),\n",
    "    StructField('rating', DoubleType())\n",
    "])\n",
    "\n",
    "rating = spark.read.csv(path_rating, schema = rating_schema, header = True)\n",
    "#rating = rating.loc[rating['rating'] != -1]\n",
    "rating = rating.filter(col('rating') != -1)\n",
    "rating = rating.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_selection = pd.DataFrame(rating['user_id'].unique()).sample(frac = .2, random_state = 63)\n",
    "#new_rating = rating[rating['user_id'].isin(random_selection[0])]\n",
    "#new_rating\n",
    "new_rating = rating.sample(withReplacement=False, fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the 20% of the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_random = new_rating.sample(frac = .2, random_state = 63) # for the sake of this exercise, going to use only 20% of the dataset due to size\n",
    "#split_size = int(0.8*len(df_random)) # designate split size (80%)\n",
    "#train_df = df_random[:split_size] # split dataset into 80% train and 20% test\n",
    "#test_df = df_random[split_size:]\n",
    "#train_df = pd.DataFrame(train_df)\n",
    "#test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_df, test_df = new_rating.randomSplit([0.8,0.2], seed = 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|anime_id|rating|\n",
      "+-------+--------+------+\n",
      "|      1|   11617|  10.0|\n",
      "|      3|    1564|   7.0|\n",
      "|      3|   16894|  10.0|\n",
      "|      5|      67|   6.0|\n",
      "|      5|     152|   4.0|\n",
      "|      5|     225|   1.0|\n",
      "|      5|     371|   3.0|\n",
      "|      5|     896|   4.0|\n",
      "|      5|    1313|   6.0|\n",
      "|      5|    1668|   2.0|\n",
      "|      5|    2144|   1.0|\n",
      "|      5|   16694|   5.0|\n",
      "|      5|   16918|   7.0|\n",
      "|      5|   18465|   2.0|\n",
      "|      5|   19769|   3.0|\n",
      "|      5|   20053|   3.0|\n",
      "|      5|   20767|   7.0|\n",
      "|      5|   23079|   2.0|\n",
      "|      5|   24873|   1.0|\n",
      "|      7|     170|   9.0|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.808821379896103\n",
      "+-------+-----------------+\n",
      "|user_id|        user_bias|\n",
      "+-------+-----------------+\n",
      "|      1|             10.0|\n",
      "|      3|              8.5|\n",
      "|      5|           3.5625|\n",
      "|      7|7.722222222222222|\n",
      "|     10|              9.0|\n",
      "|     11|8.666666666666666|\n",
      "|     12|7.333333333333333|\n",
      "|     14|7.166666666666667|\n",
      "|     16|              8.0|\n",
      "|     17|6.666666666666667|\n",
      "|     18|             10.0|\n",
      "|     19|             10.0|\n",
      "|     20|             10.0|\n",
      "|     21|              6.5|\n",
      "|     23|             10.0|\n",
      "|     24|8.666666666666666|\n",
      "|     25|              8.0|\n",
      "|     27|8.714285714285714|\n",
      "|     29|              7.0|\n",
      "|     30|              9.0|\n",
      "+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "+--------+-----------------+\n",
      "|anime_id|       anime_bias|\n",
      "+--------+-----------------+\n",
      "|       1|8.814885496183207|\n",
      "|       5|8.493212669683258|\n",
      "|       6|8.390547263681592|\n",
      "|       7|7.714285714285714|\n",
      "|       8|7.466666666666667|\n",
      "|      15|8.391304347826088|\n",
      "|      16|8.333333333333334|\n",
      "|      17|8.421052631578947|\n",
      "|      18|8.253731343283581|\n",
      "|      19| 8.95483870967742|\n",
      "|      20|7.885875706214689|\n",
      "|      22|             8.08|\n",
      "|      24|8.152073732718893|\n",
      "|      25|7.746031746031746|\n",
      "|      26|8.016666666666667|\n",
      "|      27|7.449367088607595|\n",
      "|      28|8.295081967213115|\n",
      "|      29|8.285714285714286|\n",
      "|      30|8.459770114942529|\n",
      "|      31|7.763736263736264|\n",
      "+--------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_means = train_df.select(mean('rating')).collect()[0][0] \n",
    "print(train_means)\n",
    "\n",
    "user_bias = train_df.groupBy(\"user_id\").mean(\"rating\").orderBy(\"user_id\")\n",
    "user_bias = user_bias.withColumnRenamed(\"avg(Rating)\",\"user_bias\")\n",
    "user_bias.show()\n",
    "\n",
    "anime_bias = train_df.groupBy(\"anime_id\").mean(\"rating\").orderBy(\"anime_id\")\n",
    "anime_bias = anime_bias.withColumnRenamed(\"avg(Rating)\",\"anime_bias\")\n",
    "anime_bias.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys   \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def get_selenium():                           \n",
    "    options = webdriver.ChromeOptions()                      \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return (driver)\n",
    "\n",
    "anime_dict = {}\n",
    "\n",
    "def get_summaries(url):\n",
    "\n",
    "    driver.get(url) \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    summary = soup.find_all(\"div\", {\"class\" : \"synopsis js-synopsis\"}) # this is anime descriptions\n",
    "    anime_id = soup.find_all(\"div\", {\"class\" : \"genres js-genre\"}, id=True) # this is anime id\n",
    "    \n",
    "    for i, j in zip(anime_id, summary):\n",
    "        anime_dict[i[\"id\"]] = j.get_text().strip().replace('\\n',' ').replace('\\r', ' ') # anime id, descriptions in dict\n",
    "\n",
    "base_url = \"https://myanimelist.net/anime/genre/\"\n",
    "more_pages = True\n",
    "for a in range(83):\n",
    "    if a+1 in [9,49,12]: # not family friendly\n",
    "        continue\n",
    "    driver = get_selenium() # new driver each time because of bot detection \n",
    "    genre_url = base_url + str(a+1)\n",
    "    get_summaries(genre_url)\n",
    "    time.sleep(2)\n",
    "    page_index = 2\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    driver.execute_script(f\"window.scrollTo(0, {screen_height / 2});\")\n",
    "    while more_pages:\n",
    "        try: # is current page 404?\n",
    "            driver.find_element(By.CSS_SELECTOR, \"p.message\") # 404 message - if it exists, then there are no more pages\n",
    "            more_pages = False\n",
    "        except: # if current page is not 404 then move onto next page\n",
    "            full_url = genre_url + \"?page=\" + str(page_index) # iterate to next page\n",
    "            get_summaries(full_url)\n",
    "            page_index += 1\n",
    "            full_url = genre_url\n",
    "            time.sleep(2) # website has bot detection\n",
    "            screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "            driver.execute_script(f\"window.scrollTo(0, {screen_height / 2});\")\n",
    "    full_url = base_url\n",
    "    \n",
    "anime_dataframe = pd.DataFrame.from_dict(anime_dict, orient='index') # dict to dataframe with index as keys\n",
    "anime_dataframe.to_csv(\"anime_summaries.csv\",sep =';',index=True) # export to CSV\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
