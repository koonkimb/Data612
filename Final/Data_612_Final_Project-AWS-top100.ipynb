{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5908ff-23d2-4e0c-9634-b18cd6e8bdf5",
   "metadata": {},
   "source": [
    "# Final Project - AWS\n",
    "\n",
    "Originally, the plan was to run the code with Jupyter notebook on a single AWS EC2 instance.  The specs available in the free tier were not conducive to this plan, as they were worse than the specs available in Google Colab.  Spark on Google Colab was already an improvement over local Jupyter Notebook and Pandas - the Spark dataframes did allow me to create a global baseline recommender with the full user-ratings dataset (although the code above only uses a fraction, as the ultimate goal was to move to AWS resources).  Google Colab was also limited in its runtime as it would disconnect after around half a day - anything running longer than this would be lost.  \n",
    "\n",
    "To solve this issue, I ended up creating a Spark cluster composed of various EC2 instances.  The first step was to upload my documents into an S3 bucket.  \n",
    "\n",
    "I created an S3 bucket called data-612-kkoon, where I placed the anime_summaries.csv, anime.csv, and ratings.csv objects.  This process was fairly straightforward.  Bucket versioning was disabled as I would only be reading from this bucket.  I did accidentally create this bucket in the wrong region at first - I deleted the original bucket and created a new one in the correct region.  However, another option could be to clone the S3 bucket across regions with S3-Cross Region Replication.  Since I only had three files uploaded, I chose the simpler route which was to recreate the bucket.  \n",
    "\n",
    "The next step was to create the master EC2 instance.  I used the Amazon Linux 2023 kernel-6.1 AMI given its compatibility with PySpark - I had issues running PySpark locally on my Windows computer as it crashed with any matrix operation.  Google Colab is also Linux based.  \n",
    "\n",
    "There were four instance types available on free tier:\n",
    "\n",
    "1.  t3.micro, with 2 vCPUs and 1 GiB memory\n",
    "2.  t3.small, with 2 vCPUs and 2 GiB memory\n",
    "3.  c7i-flex.large, with 2vCPUs and 4 GiB memory\n",
    "4.  m7i-flex.large, with 2vCPU and 8 GiB memory\n",
    "\n",
    "At the time, I did not know that this instance would be my master instance with Spark.  As such, I chose the m7i-flex.large instance type, for the 8GiB memory given the large dataset and its issues with memory in previous projects.  However, since computation is done on the worker nodes, the m7i-flex.large is potentially not a necessary choice for the master node.  If I were to consider cost savings later on, I could try to downsize this EC2 instance.  Currently it is still using the m7i-flex.large instance type, like all of the workers in my Spark cluster.  \n",
    "\n",
    "I created a VPC and subnet for this project, to ensure private access to my data.  All EC2 instances used this VPC and subnet.  For my master node, inbound rules were created to allow a custom TCP from this VPC through specified ports.  This also ended up unnecessary as I set up and used an SSH tunnel to allow access to Jupyter Notebook, Spark Master, and Spark UI on my local computer.  I also added an inbound rule to allow SSH access from my IP.  \n",
    "\n",
    "I created a second security group for the worker nodes, which included inbound rules for SSH access from my IP, custom TCP from the VPC for ports 8081 (Spark worker UI) and 7337 (Spark Shuffle Service, as I had to manually start it on each worker due to my Spark installation), and a custom TCP pointing to the master node security group to allow communication from Spark Master (port 7077).  For the master security group, I added three inbound rules to allow the worker nodes to communicate with the master nodes (7077 for Spark Master port, 7078 for Spark Driver port, 7079 for Spark Block manager port).  For future steps, I would re-assess the need for the inbound rule for port 8081 for the worker security group and 7077 for the master security group.  I had incrementally added these rules to fix an issue that was preventing the master node from communicating with the worker nodes, so there may be extraneous, unnecessary inbound rules configured.  \n",
    "\n",
    "To access the EC2 instances, I used PuTTY to SSH into these instances with private key authentication credentials.  I used the same private key for all nodes.  For the master node, I created an SSH tunnel to Jupyter Notebook, the master Spark UI, and the PySpark UI (ports 8888, 8080, and 4040, respectively).  All nodes used the same default user login, ec2-user.  \n",
    "\n",
    "To set up the master node, I installed Jupyter Notebook.  When launching Jupyter Notebook, I can use the specified token to access it on my local computer via the SSH tunnel set up in the previous step.  For all nodes, I installed Java, Spark, and Scikit-Learn.  I set up the JAVE_HOME and SPARK_HOME environment variables globally.  \n",
    "\n",
    "In AWS, I created an IAM role with S3 admin access.  I assigned this IAM role to all the nodes to allow it to access the S3 bucket.  \n",
    "\n",
    "To create and connect the Spark Cluster, I started the master node and then all worker nodes.  I checked the Master UI on my local computer via the SSH tunnel and confirmed the worker connection.  I also manually started shuffle services on each worker node, which I enable in my Jupyter Notebook.\n",
    "\n",
    "I began with 3 workers but after attempting to run my recommender system, I was receiving errors related to disk space.  I created a custom AMI image from one of my workers and launched 7 new workers with 32GiB EBS each, using the AMI image so that all the installs were already available for my new workers.  AWS free tier has a 16 total core limitation, which meant my Spark cluster was maxed out at 7 workers and 1 master.  \n",
    "\n",
    "After some testing, I realized that my problem was not computational power, but memory.  Additionally, the large number of workers resulted in too much data shuffling.  This caused the Spark cluster to run my code even slower than in Google Colab.  After making adjustments to how I cached and unpersisted dataframes, I used only one worker with 2 vCPU, 8GiB RAM, and 108 EBS to allow persisted dataframes to spill into disk if required.  The Spark cluster set up (e.g. master node - worker node) was not necessary, but since my code was set up to use it and my master node had only 8GiB EBS, I continued with this configuration.  \n",
    "\n",
    "Additionally, after examining my code, I realized that the cross-join for cosine similarity was resulting in disproportionate computation burden compared to added value.   I sorted the anime by number of user ratings and sorted by count of ratings.  I found that I could have 1,000,000 user-anime ratings with only 100 anime. Since the computation time of the cross join would increase exponentially, I chose to run the code for 100 of the top animes, which was over 1,000,000 user ratings to fulfil project requirements.\n",
    "\n",
    "With everything set-up, I can now run my code with the following PySpark configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c2d657-c96a-4013-8dfb-a97343dc27b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1a219bbc-b684-42ff-a958-521c122332fa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 264ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1a219bbc-b684-42ff-a958-521c122332fa\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/07/21 01:48:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.218:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.0.0.218:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f83780091c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-amazon-corretto'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ec2-user/sklearn-env/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/ec2-user/sklearn-env/bin/python\"\n",
    "\n",
    "# Initialize findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.0.0.218:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.InstanceProfileCredentialsProvider\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .config(\"spark.driver.host\", \"10.0.0.218\") \\\n",
    "    .config(\"spark.driver.port\", \"7078\") \\\n",
    "    .config(\"spark.blockManager.port\", \"7079\") \\\n",
    "    .config(\"spark.driver.host\", \"10.0.0.218\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"10.0.0.218\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/home/ec2-user/sklearn-env/bin/python\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e04219-e8b2-45a2-8bb6-e86224666426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import IntegerType,StringType,StructField,StructType, BooleanType, FloatType\n",
    "from pyspark.sql.functions import count, concat_ws, col, lit, mean, count_distinct, when, split, udf, coalesce, when, sum, abs, first\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, Normalizer\n",
    "from pyspark.ml.linalg import DenseVector, Vectors\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b8c7e2-5757-44d9-86b8-2981e5dd1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:48:53 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.808683581523678\n",
      "+-------+--------------------+\n",
      "|user_id|           user_bias|\n",
      "+-------+--------------------+\n",
      "|     12|   1.080205307365211|\n",
      "|     14| -0.6328594056995023|\n",
      "|     18|   1.401842734265796|\n",
      "|     38| -1.1453172448900153|\n",
      "|     46|  0.7481791635743607|\n",
      "|     67|  1.1913164184763216|\n",
      "|     70|  2.1913164184763216|\n",
      "|     93| -0.3136340765731829|\n",
      "|    107| 0.19131641847632164|\n",
      "|    148| -0.3570706782978723|\n",
      "|    161|  1.1913164184763216|\n",
      "|    171|  1.4720181728622874|\n",
      "|    186|  0.5327798331104683|\n",
      "|    190| -0.6486835815236782|\n",
      "|    198|  2.0484592756191793|\n",
      "|    202|-0.39489047807540256|\n",
      "|    203|  1.1913164184763216|\n",
      "|    218|  1.1913164184763216|\n",
      "|    225|  0.3264515536114567|\n",
      "|    232|-0.39489047807540256|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+--------------------+\n",
      "|anime_id|          anime_bias|\n",
      "+--------+--------------------+\n",
      "|   11771|  0.8060012787752173|\n",
      "|    1943|  0.4000044778520033|\n",
      "|   16782|  0.6166037747981603|\n",
      "|   17265|  0.4807344078943112|\n",
      "|   20021| -0.5418674380259203|\n",
      "|   20507| 0.40914215788479513|\n",
      "|   23301| 0.12832429249206978|\n",
      "|   28701|  0.7733486068020481|\n",
      "|     186|  0.3736934676566497|\n",
      "|     190| -0.3542425401760516|\n",
      "|     225| -0.9396735581816493|\n",
      "|     232| 0.42328807333251195|\n",
      "|     263|  1.1500883483008826|\n",
      "|     406|  -0.921524048449748|\n",
      "|     443| -0.2585480801686648|\n",
      "|     987|  -0.920167236175816|\n",
      "|    1195|-0.02984499884651...|\n",
      "|    2236|  0.7557927584628663|\n",
      "|    2418|  0.6628175471444928|\n",
      "|    2508| 0.16496061513116178|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_desc_schema = StructType([\n",
    "    StructField('anime_id', IntegerType()),\n",
    "    StructField('description', StringType())\n",
    "])\n",
    "anime_descriptions = spark.read.option(\"sep\", \";\").csv('s3a://data-612-kkoon/anime_summaries.csv', schema = anime_desc_schema)\n",
    "anime_descriptions = anime_descriptions.filter(col(\"anime_id\").isNotNull())\n",
    "schema = StructType([\n",
    "    StructField('anime_id', IntegerType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('genre', StringType()),\n",
    "    StructField('type', StringType()),\n",
    "    StructField('episodes', FloatType()),\n",
    "    StructField('rating', FloatType()),\n",
    "    StructField('members', IntegerType())\n",
    "])\n",
    "anime = spark.read.csv('s3a://data-612-kkoon/anime.csv', schema = schema, header = True)\n",
    "anime_full = anime.join(anime_descriptions, on = \"anime_id\", how = \"inner\")\n",
    "\n",
    "rating_schema = StructType([\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('anime_id', IntegerType()),\n",
    "    StructField('rating', FloatType())\n",
    "])\n",
    "rating = spark.read.csv('s3a://data-612-kkoon/rating.csv', schema = rating_schema, header = True)\n",
    "rating = rating.filter(col('rating') != -1)\n",
    "rating = rating.dropna()\n",
    "\n",
    "rating = rating.cache()\n",
    "#new_rating = rating.select(\"user_id\").distinct().sample(False, 0.15, seed=63) # meet project requirements - at least 10k users\n",
    "#new_rating = new_rating.join(rating, on=\"user_id\", how = \"inner\")\n",
    "train_df, test_df = rating.randomSplit([0.8,0.2], seed = 63)\n",
    "train_df_cache = train_df.cache()\n",
    "\n",
    "train_means = train_df.select(mean('rating')).collect()[0][0]\n",
    "print(train_means)\n",
    "\n",
    "user_bias = train_df.groupBy(\"user_id\").mean(\"rating\")\n",
    "user_bias = user_bias.withColumnRenamed(\"avg(Rating)\",\"user_bias\")\n",
    "user_bias = user_bias.withColumn(\"user_bias\", user_bias[\"user_bias\"] - train_means)\n",
    "\n",
    "user_bias_cached = user_bias.cache()\n",
    "user_bias_cached.show()\n",
    "\n",
    "anime_bias = train_df.groupBy(\"anime_id\").mean(\"rating\")\n",
    "anime_bias = anime_bias.withColumnRenamed(\"avg(Rating)\",\"anime_bias\")\n",
    "anime_bias = anime_bias.withColumn(\"anime_bias\", anime_bias[\"anime_bias\"] - train_means)\n",
    "\n",
    "anime_bias_cached = anime_bias.cache()\n",
    "anime_bias_cached.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1628a-b85e-41d3-95b8-59b20ed73157",
   "metadata": {},
   "source": [
    "# Global Baseline Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c76faee-fded-43c7-a4d1-aca543a6aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+--------------------+------------------+-----------------+\n",
      "|anime_id|user_id|rating|           user_bias|        anime_bias|       prediction|\n",
      "+--------+-------+------+--------------------+------------------+-----------------+\n",
      "|      18|    202|   8.0|-0.39489047807540256|0.5148950137940469|7.928688117242323|\n",
      "|      18|   2115|   7.0| -0.4407906049350494|0.5148950137940469|7.882787990382676|\n",
      "|      18|   2501|   9.0|  0.7839090110689151|0.5148950137940469|9.107487606386641|\n",
      "|      18|   3196|  10.0|  0.9034376305975345|0.5148950137940469| 9.22701622591526|\n",
      "|      18|   3781|   6.0| -0.4299460399954391|0.5148950137940469|7.893632555322286|\n",
      "|      18|   4662|   9.0| 0.10481122816490274|0.5148950137940469|8.428389823482629|\n",
      "|      18|   5427|   9.0|  1.2079830851429891|0.5148950137940469|9.531561680460715|\n",
      "|      18|   5767|   8.0| -1.0434661902193305|0.5148950137940469|7.280112405098395|\n",
      "|      18|   6118|   9.0|  1.1622707753227948|0.5148950137940469| 9.48584937064052|\n",
      "|      18|   6237|  10.0|  -1.579447701125007|0.5148950137940469|6.744130894192718|\n",
      "|      18|   6474|   7.0| -1.9987772896896754|0.5148950137940469| 6.32480130562805|\n",
      "|      18|   6968|   9.0|  -1.044491441785687|0.5148950137940469|7.279087153532038|\n",
      "|      18|   7009|  10.0|   1.294409201981476|0.5148950137940469|  9.6179877972992|\n",
      "|      18|   7674|  10.0|  0.7492874329690755|0.5148950137940469|  9.0728660282868|\n",
      "|      18|   7841|  10.0|   1.746871974031877|0.5148950137940469|             10.0|\n",
      "|      18|   7939|   7.0|0.046520038385823526|0.5148950137940469|8.370098633703549|\n",
      "|      18|   8044|   7.0| -0.2938906821153946|0.5148950137940469| 8.02968791320233|\n",
      "|      18|  10278|   8.0| -0.1748807646222703|0.5148950137940469|8.148697830695454|\n",
      "|      18|  10280|  10.0|  1.3615291844337678|0.5148950137940469|9.685107779751494|\n",
      "|      18|  10479|   7.0|-0.11303140761063446|0.5148950137940469| 8.21054718770709|\n",
      "+--------+-------+------+--------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "global_baseline_train = train_df_cache.join(user_bias_cached, on = \"user_id\", how = \"left\")\n",
    "global_baseline_train = global_baseline_train.join(anime_bias_cached, on = \"anime_id\", how = \"left\")\n",
    "\n",
    "global_baseline_train = global_baseline_train.withColumn(\"prediction\",\n",
    "                                                         coalesce(global_baseline_train[\"user_bias\"],lit(0)) +\n",
    "                                                         coalesce(global_baseline_train[\"anime_bias\"],lit(0)) +\n",
    "                                                         train_means)\n",
    "\n",
    "global_baseline_train = global_baseline_train.withColumn(\"prediction\", when(global_baseline_train[\"prediction\"] < 1, 1)\n",
    "    .when(global_baseline_train[\"prediction\"] > 10, 10)\n",
    "    .otherwise(global_baseline_train[\"prediction\"]))\n",
    "\n",
    "global_baseline_train_cache = global_baseline_train.cache()\n",
    "global_baseline_train_cache.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac68a66b-4e3f-41ca-a0f5-4ccf365fbe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===========================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2117542539704813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "rmse_gb_train = evaluator.evaluate(global_baseline_train_cache)\n",
    "print(rmse_gb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c187809-2299-4a21-8467-78159656b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+--------------------+--------------------+------------------+\n",
      "|anime_id|user_id|rating|           user_bias|          anime_bias|        prediction|\n",
      "+--------+-------+------+--------------------+--------------------+------------------+\n",
      "|   11617|      1|  10.0|  2.1913164184763216|-0.03175168038695908| 9.968248319613041|\n",
      "|     170|      3|   9.0|-0.33571060855070556|  0.8107806065572403| 8.283753579530213|\n",
      "|     225|      3|   9.0|-0.33571060855070556| -0.9396735581816493|6.5332994147913235|\n",
      "|     813|      3|  10.0|-0.33571060855070556|  0.5133861122461214| 7.986359085219094|\n",
      "|    1119|      3|   7.0|-0.33571060855070556| -0.6946402261985138| 6.778332746774459|\n",
      "|    1292|      3|   6.0|-0.33571060855070556|-0.31398312069418566| 7.158989852278787|\n",
      "|    1564|      3|   7.0|-0.33571060855070556| -0.5502909060404635| 6.922682066932509|\n",
      "|    2201|      3|   7.0|-0.33571060855070556| -0.7775417130115674| 6.695431259961405|\n",
      "|    7695|      3|   7.0|-0.33571060855070556| -0.6144291355592459| 6.858543837413727|\n",
      "|    9135|      3|   7.0|-0.33571060855070556|-0.12630653234335032|7.3466664406296225|\n",
      "|    9760|      3|   9.0|-0.33571060855070556| 0.09878395094385439| 7.571756923916827|\n",
      "|   11771|      3|  10.0|-0.33571060855070556|  0.8060012787752173|  8.27897425174819|\n",
      "|   12671|      3|   3.0|-0.33571060855070556| -0.9358229458268568| 6.537150027146116|\n",
      "|   20159|      3|  10.0|-0.33571060855070556| 0.24387116300186928| 7.716844135974842|\n",
      "|   21881|      3|   7.0|-0.33571060855070556|-0.15933151792790312|  7.31364145504507|\n",
      "|   22319|      3|   6.0|-0.33571060855070556| 0.32389564163193985| 7.796868614604913|\n",
      "|   28171|      3|  10.0|-0.33571060855070556|     0.8490320704903| 8.322005043463273|\n",
      "|   28891|      3|   9.0|-0.33571060855070556|    1.12533625066549| 8.598309223638463|\n",
      "|   31043|      3|  10.0|-0.33571060855070556|  0.9355900300232305| 8.408563002996203|\n",
      "|      18|      5|   6.0| -3.4655200426496835|  0.5148950137940469| 4.858058552668042|\n",
      "+--------+-------+------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_baseline_test = test_df.join(user_bias_cached, on = \"user_id\", how = \"left\")\n",
    "global_baseline_test = global_baseline_test.join(anime_bias_cached, on = \"anime_id\", how = \"left\")\n",
    "\n",
    "global_baseline_test = global_baseline_test.withColumn(\"prediction\",\n",
    "                                                       coalesce(global_baseline_test[\"user_bias\"],lit(0)) +\n",
    "                                                       coalesce(global_baseline_test[\"anime_bias\"],lit(0)) +\n",
    "                                                       train_means)\n",
    "global_baseline_test = global_baseline_test.withColumn(\"prediction\", when(global_baseline_test[\"prediction\"] < 1, 1)\n",
    "    .when(global_baseline_test[\"prediction\"] > 10, 10)\n",
    "    .otherwise(global_baseline_test[\"prediction\"]))\n",
    "\n",
    "global_baseline_test_cache = global_baseline_test.cache()\n",
    "global_baseline_test_cache.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e4c1d4-3409-442f-ba0c-e8c450932b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================>                     (5 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2297122835232495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "rmse_gb_test = evaluator.evaluate(global_baseline_test_cache)\n",
    "print(rmse_gb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3426c-5231-4953-8eec-555bce5d52e0",
   "metadata": {},
   "source": [
    "# Content Based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e71bc4-5678-48c8-bd44-f0125e155af6",
   "metadata": {},
   "source": [
    "When calculating cosine similarity, computation time increases exponentially due to the cross join.  Cosine similarity was identified as a bottleneck, as it would take hours on just the cosine similarity step.  An anime with only one rating could be disproportionately affecting computation time in comparison to the value it adds.  Given that 1 million ratings is the target for this project, I will compute the cosine similarity for only the top 100 anime.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356f2428-1f35-43db-9567-830d3ef088e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sum(user_count)|\n",
      "+---------------+\n",
      "|        1345994|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, anime_id: int, rating: float]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.groupBy(\"anime_id\").agg(count(\"user_id\").alias(\"user_count\")).orderBy(col(\"user_count\").desc()).limit(100).agg(sum(\"user_count\")).show()\n",
    "anime_pared = rating.groupBy(\"anime_id\").agg(count(\"user_id\").alias(\"user_count\")).orderBy(col(\"user_count\").desc()).limit(100).select(\"anime_id\")\n",
    "rating.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fc7bfc-5f38-4dba-8196-c2696083357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----+--------+------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|anime_id|                name|               genre|type|episodes|rating|members|         description|               words|          words_nosw|            features|\n",
      "+--------+--------------------+--------------------+----+--------+------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   16498|  Shingeki no Kyojin|Action, Drama, Fa...|  TV|    25.0|  8.54| 896229|Centuries ago, ma...|[centuries, ago,,...|[centuries, ago,,...|(5000,[78,122,175...|\n",
      "|    5114|Fullmetal Alchemi...|Action, Adventure...|  TV|    64.0|  9.26| 793665|After a horrific ...|[after, a, horrif...|[horrific, alchem...|(5000,[49,152,342...|\n",
      "|   30276|       One Punch Man|Action, Comedy, P...|  TV|    12.0|  8.82| 552458|The seemingly uni...|[the, seemingly, ...|[seemingly, unimp...|(5000,[1,20,83,14...|\n",
      "|   11757|    Sword Art Online|Action, Adventure...|  TV|    25.0|  7.83| 893100|Ever since the re...|[ever, since, the...|[ever, since, rel...|(5000,[105,108,11...|\n",
      "|      20|              Naruto|Action, Comedy, M...|  TV|   220.0|  7.81| 683297|Moments before Na...|[moments, before,...|[moments, naruto,...|(5000,[89,133,188...|\n",
      "|   22319|         Tokyo Ghoul|Action, Drama, Ho...|  TV|    12.0|  8.07| 618056|A sinister threat...|[a, sinister, thr...|[sinister, threat...|(5000,[78,87,189,...|\n",
      "|    1575|Code Geass: Hangy...|Action, Mecha, Mi...|  TV|    25.0|  8.83| 715151|In the year 2010,...|[in, the, year, 2...|[year, 2010,, hol...|(5000,[43,242,378...|\n",
      "|   20507|            Noragami|Action, Adventure...|  TV|    12.0|  8.17| 515378|In times of need,...|[in, times, of, n...|[times, need,, lo...|(5000,[2,46,116,1...|\n",
      "|   22199|      Akame ga Kill!|Action, Adventure...|  TV|    24.0|  7.84| 492133|Night Raid is the...|[night, raid, is,...|[night, raid, cov...|(5000,[63,157,171...|\n",
      "|     269|              Bleach|Action, Comedy, S...|  TV|   366.0|  7.95| 624055|Ichigo Kurosaki i...|[ichigo, kurosaki...|[ichigo, kurosaki...|(5000,[58,92,96,1...|\n",
      "|   10620|    Mirai Nikki (TV)|Action, Mystery, ...|  TV|    26.0|  8.07| 657190|Yukiteru Amano is...|[yukiteru, amano,...|[yukiteru, amano,...|(5000,[133,233,23...|\n",
      "|   21881| Sword Art Online II|Action, Adventure...|  TV|    24.0|  7.35| 537892|A year after esca...|[a, year, after, ...|[year, escaping, ...|(5000,[15,117,285...|\n",
      "|    9919|      Ao no Exorcist|Action, Demons, F...|  TV|    25.0|  7.92| 583823|Humans and demons...|[humans, and, dem...|[humans, demons, ...|(5000,[40,78,102,...|\n",
      "|       1|        Cowboy Bebop|Action, Adventure...|  TV|    26.0|  8.82| 486824|Crime is timeless...|[crime, is, timel...|[crime, timeless....|(5000,[13,161,231...|\n",
      "|   22535|Kiseijuu: Sei no ...|Action, Drama, Ho...|  TV|    24.0|  8.59| 425457|All of a sudden, ...|[all, of, a, sudd...|[sudden,, arrived...|(5000,[12,102,150...|\n",
      "|      30|Neon Genesis Evan...|Action, Dementia,...|  TV|    26.0|  8.32| 461946|Fifteen years aft...|[fifteen, years, ...|[fifteen, years, ...|(5000,[11,20,49,1...|\n",
      "|    2904|Code Geass: Hangy...|Action, Drama, Me...|  TV|    25.0|  8.98| 572888|One year has pass...|[one, year, has, ...|[one, year, passe...|(5000,[49,116,302...|\n",
      "|   27899|      Tokyo Ghoul √A|Action, Drama, Ho...|  TV|    12.0|  7.52| 408357|Ken Kaneki has fi...|[ken, kaneki, has...|[ken, kaneki, fin...|(5000,[19,67,92,1...|\n",
      "|    6702|          Fairy Tail|Action, Adventure...|  TV|   175.0|  8.22| 584590|In the enchanted ...|[in, the, enchant...|[enchanted, kingd...|(5000,[15,49,188,...|\n",
      "|   18679|        Kill la Kill|Action, Comedy, S...|  TV|    24.0|  8.23| 508118|After the murder ...|[after, the, murd...|[murder, father,,...|(5000,[99,103,211...|\n",
      "+--------+--------------------+--------------------+----+--------+------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "new_anime_full = anime_full.join(anime_pared, on = \"anime_id\", how = \"inner\")\n",
    "\n",
    "new_anime_full = new_anime_full.withColumn(\"description\", F.regexp_replace(\"description\", '\\\"', '')) # remove quotes\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "anime_df_processed = tokenizer.transform(new_anime_full)\n",
    "\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_nosw\", stopWords=stop_words)\n",
    "anime_df_processed_nosw = remover.transform(anime_df_processed)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words_nosw\", outputCol=\"features\", numFeatures=5000) #numFeatures=500\n",
    "anime_features_df = hashingTF.transform(anime_df_processed_nosw)\n",
    "\n",
    "anime_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68253abb-3cb2-4a90-967e-9bf2cbee8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|anime_id|        IDF_features|\n",
      "+--------+--------------------+\n",
      "|   16498|(5000,[78,122,175...|\n",
      "|    5114|(5000,[49,152,342...|\n",
      "|   30276|(5000,[1,20,83,14...|\n",
      "|   11757|(5000,[105,108,11...|\n",
      "|      20|(5000,[89,133,188...|\n",
      "|   22319|(5000,[78,87,189,...|\n",
      "|    1575|(5000,[43,242,378...|\n",
      "|   20507|(5000,[2,46,116,1...|\n",
      "|   22199|(5000,[63,157,171...|\n",
      "|     269|(5000,[58,92,96,1...|\n",
      "|   10620|(5000,[133,233,23...|\n",
      "|   21881|(5000,[15,117,285...|\n",
      "|    9919|(5000,[40,78,102,...|\n",
      "|       1|(5000,[13,161,231...|\n",
      "|   22535|(5000,[12,102,150...|\n",
      "|      30|(5000,[11,20,49,1...|\n",
      "|    2904|(5000,[49,116,302...|\n",
      "|   27899|(5000,[19,67,92,1...|\n",
      "|    6702|(5000,[15,49,188,...|\n",
      "|   18679|(5000,[99,103,211...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"features\", outputCol=\"IDF_features\")\n",
    "idf_model = idf.fit(anime_features_df)\n",
    "tfidf_df = idf_model.transform(anime_features_df)\n",
    "tfidf_df = tfidf_df.select(\"anime_id\",\"IDF_features\")\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55905672-091e-4c60-89fd-1823295ffbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|anime_id|anime_id_2| similarity|\n",
      "+--------+----------+-----------+\n",
      "|   16498|      5114|0.046655748|\n",
      "|   16498|     30276| 0.04346148|\n",
      "|   16498|     11757|0.048580788|\n",
      "|   16498|        20|0.041770328|\n",
      "|   16498|     22319| 0.03133955|\n",
      "|   16498|      1575|0.018288309|\n",
      "|   16498|     20507| 0.04277198|\n",
      "|   16498|     22199|0.023875764|\n",
      "|   16498|       269|0.011576882|\n",
      "|   16498|     10620|0.025270337|\n",
      "|   16498|     21881|0.052557766|\n",
      "|   16498|      9919| 0.02926118|\n",
      "|   16498|         1|0.016164998|\n",
      "|   16498|     22535|0.015055798|\n",
      "|   16498|        30| 0.07857197|\n",
      "|   16498|      2904|0.022364216|\n",
      "|   16498|     27899|0.012461833|\n",
      "|   16498|      6702| 0.00723235|\n",
      "|   16498|     18679|0.044972453|\n",
      "|   16498|      3588|0.022739379|\n",
      "+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(inputCol=\"IDF_features\", outputCol=\"norm\")\n",
    "normalized_tfidf_df = normalizer.transform(tfidf_df)\n",
    "normalized_tfidf_df_2 = normalized_tfidf_df.withColumnRenamed(\"anime_id\", \"anime_id_2\").withColumnRenamed(\"norm\",\"norm_2\")\n",
    "#normalized_tfidf_df = normalized_tfidf_df.repartition(\"anime_id\")\n",
    "#normalized_tfidf_df_2 = normalized_tfidf_df_2.repartition(\"anime_id_2\")\n",
    "\n",
    "sim_cos = udf(lambda x,y : float(x.dot(y)), FloatType())\n",
    "\n",
    "cosine_sim = (\n",
    "    normalized_tfidf_df\n",
    "    .crossJoin(normalized_tfidf_df_2)\n",
    "    .filter(normalized_tfidf_df[\"anime_id\"] != normalized_tfidf_df_2[\"anime_id_2\"])\n",
    "    .withColumn(\"similarity\", sim_cos(normalized_tfidf_df[\"norm\"], normalized_tfidf_df_2[\"norm_2\"]))\n",
    "    .select(\n",
    "        \"anime_id\",\n",
    "        \"anime_id_2\",\n",
    "        \"similarity\"\n",
    "    )\n",
    ")\n",
    "\n",
    "cosine_sim_cache = cosine_sim.cache()\n",
    "cosine_sim_cache.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b5c30a-c539-4f82-a266-93eae3e98a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pared = global_baseline_train_cache.join(cosine_sim_cache, on=\"anime_id\", how = \"inner\") \n",
    "global_baseline_train.unpersist()\n",
    "\n",
    "test_df_pared = global_baseline_test_cache.join(cosine_sim_cache, on=\"anime_id\", how = \"inner\")\n",
    "global_baseline_test.unpersist()\n",
    "\n",
    "train_df_pared = train_df_pared.withColumn(\"rating\", train_df_pared[\"rating\"] - train_df_pared[\"prediction\"])\n",
    "train_df_pared = train_df_pared.select(col(\"user_id\"),col(\"anime_id\"),col(\"anime_id_2\"),col(\"rating\"),col(\"similarity\"),col(\"prediction\").alias(\"global_baseline\"))\n",
    "train_df_pared_cache = train_df_pared.cache()\n",
    "\n",
    "test_df_pared = test_df_pared.withColumn(\"rating\", test_df_pared[\"rating\"] - test_df_pared[\"prediction\"])\n",
    "test_df_pared = test_df_pared.select(col(\"user_id\"),col(\"anime_id\"),col(\"anime_id_2\"),col(\"rating\"),col(\"similarity\"),col(\"prediction\").alias(\"global_baseline\"))\n",
    "test_df_pared_cache = test_df_pared.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a89d7c-9fe0-4a00-a429-dae87d821e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------+------------------+\n",
      "|user_id|anime_id_2|        prediction|rating|   global_baseline|\n",
      "+-------+----------+------------------+------+------------------+\n",
      "|      1|     11757|              10.0|  10.0|              10.0|\n",
      "|      3|        20| 7.879086456162366|   8.0| 7.549534364544735|\n",
      "|      3|      6702| 8.495952787755837|   8.0| 8.159327057358658|\n",
      "|      3|     28223|  8.39854623447299|   6.0| 8.121219806713043|\n",
      "|      5|       225|3.4105293016905227|   1.0|3.4034899806923455|\n",
      "|      5|      9989| 4.688538711508015|   7.0|5.3123621093571485|\n",
      "|      5|     17265|  4.23228569166715|   8.0| 4.823897946768306|\n",
      "|      5|     22319| 3.849632131798173|   2.0| 4.667059180505935|\n",
      "|      7|       270| 7.486343462901233|   9.0|  7.30386699491074|\n",
      "|      7|      4654| 7.612370026240543|   9.0| 7.446378287410231|\n",
      "|      7|     10719| 7.479884012916218|   8.0|7.2816193621249905|\n",
      "|     10|     11757| 9.517858659055848|   9.0| 9.662729425913202|\n",
      "|     11|      9756| 7.948288916568576|   7.0| 8.045019918366263|\n",
      "|     11|     18153| 7.393245806286764|   9.0| 7.481006890528171|\n",
      "|     11|     30276| 8.123198844887865|   8.0| 8.418584110907949|\n",
      "|     12|      2025| 8.854323642434872|   9.0| 9.479621668091937|\n",
      "|     14|     10793|7.2767963861212985|   8.0| 7.364014333711551|\n",
      "|     14|     14345|  7.24377561698424|   7.0| 7.175349731852416|\n",
      "|     14|     19815| 7.889647431143626|   8.0| 7.959403929235625|\n",
      "|     16|      6547| 8.510285759495675|   8.0| 8.577282731949158|\n",
      "+-------+----------+------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predictions = train_df_pared_cache.withColumn( # prediction is then retrieved by taking the cosine similarity * rating for similar items\n",
    "    \"cos_simXratings\", col(\"similarity\") * col(\"rating\")\n",
    ").groupBy(\"user_id\", \"anime_id_2\").agg(\n",
    "    sum(\"cos_simXratings\").alias(\"sumCos_simXratings\"), # sum up the cosine similarity * rating\n",
    "    sum(\"similarity\").alias(\"sum_similarity\"),\n",
    ").withColumn(\n",
    "    \"prediction\",\n",
    "    when(col(\"sum_similarity\") != 0, (col(\"sumCos_simXratings\") / col(\"sum_similarity\")))\n",
    "    .otherwise(None)  # prediction is sum of cosine similarity * rating for all similar items, divided by sum of all cosine similarities for those items\n",
    ").select(\"user_id\",\"anime_id_2\",\"prediction\").distinct()\n",
    "predictions_cache = predictions.cache()\n",
    "\n",
    "train_df_renamed = train_df_pared_cache.select(\"user_id\",\"anime_id\",\"rating\",\"global_baseline\").withColumnRenamed(\"anime_id\", \"anime_id_2\").distinct() # we want to compare the predictions for \"anime_2\", but in the user-rating, this refers to the rating in anime_1\n",
    "predictions_train = predictions_cache.join(train_df_renamed, on = [\"user_id\",\"anime_id_2\"], how = \"inner\")\\\n",
    "    .withColumn(\"prediction\", coalesce(\"prediction\",lit(0)) + col(\"global_baseline\"))\\\n",
    "    .withColumn(\"rating\",col(\"rating\") + col(\"global_baseline\"))\n",
    "\n",
    "predictions_train = predictions_train.withColumn(\"prediction\", when(predictions_train[\"prediction\"] < 1, 1)\n",
    "    .when(predictions_train[\"prediction\"] > 10, 10)\n",
    "    .otherwise(predictions_train[\"prediction\"]))\n",
    "\n",
    "predictions_train_cache = predictions_train.cache()\n",
    "predictions_train_cache.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c5debb-7074-4a4c-885c-df4d8b825790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1071364"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train_cache.count() # showing number of predictions in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb91c7b5-7ac7-425b-9232-e7ae329b6333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1873520186514084\n"
     ]
    }
   ],
   "source": [
    "rmse_content_train = evaluator.evaluate(predictions_train_cache)\n",
    "print(rmse_content_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec19ea34-b032-4063-bec0-23d34d676cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------+------------------+\n",
      "|user_id|anime_id_2|        prediction|rating|   global_baseline|\n",
      "+-------+----------+------------------+------+------------------+\n",
      "|      5|      2993|3.0839422671610452|   2.0| 3.915270670306181|\n",
      "|      5|     11759| 3.908415574923569|   3.0| 4.373025211517557|\n",
      "|      5|     22535| 4.531450756594869|   7.0| 5.222082636485916|\n",
      "|      7|     11757| 7.817322762801001|   8.0| 7.693708360237862|\n",
      "|      8|     11757| 8.980118622729023|   9.0| 8.602123365307143|\n",
      "|     14|      8525| 7.108758054802281|   8.0|  7.43917935497881|\n",
      "|     14|      9253|  8.66653292086751|   9.0| 8.633943783066679|\n",
      "|     17|      9756| 7.754851295875365|   9.0|7.5668500619069325|\n",
      "|     38|     11757|  7.07335638915253|   9.0| 6.992762429213532|\n",
      "|     39|      2001| 9.792573864380389|  10.0|              10.0|\n",
      "|     44|       853| 8.178670463518923|   7.0| 8.333820291000606|\n",
      "|     46|      7054|  8.96847677214752|  10.0| 9.130095625490824|\n",
      "|     47|       199|  9.78374060246535|  10.0| 9.882630920529081|\n",
      "|     48|      2167| 8.934968533937193|   8.0| 9.396270452761438|\n",
      "|     48|      6547|  9.12533453164674|   9.0| 9.439601572528868|\n",
      "|     50|       223| 7.003289238510385|  10.0| 8.254517107032733|\n",
      "|     55|        30| 7.658377870021769|   9.0| 8.707145794465657|\n",
      "|     56|       270| 8.825731688341477|   8.0|  8.27288806058608|\n",
      "|     61|       934| 9.089957892116189|   9.0| 8.656535734533644|\n",
      "|     61|      4752| 8.961943194143446|   9.0| 7.980071586883056|\n",
      "+-------+----------+------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# could consider dropDuplicates for the following df also \n",
    "test_df_renamed = test_df_pared_cache.select(\"user_id\",\"anime_id\",\"rating\",\"global_baseline\").withColumnRenamed(\"anime_id\", \"anime_id_2\").distinct()  # but test global baseline is pulled back in, which was developed also from the training data\n",
    "predictions_test = predictions_cache.join(test_df_renamed, on = [\"user_id\",\"anime_id_2\"], how = \"inner\")\\\n",
    "    .withColumn(\"prediction\", coalesce(\"prediction\",lit(0)) + col(\"global_baseline\"))\\\n",
    "    .withColumn(\"rating\",col(\"rating\") + col(\"global_baseline\"))\n",
    "\n",
    "predictions_test = predictions_test.withColumn(\"prediction\", when(predictions_test[\"prediction\"] < 1, 1)\n",
    "    .when(predictions_test[\"prediction\"] > 10, 10)\n",
    "    .otherwise(predictions_test[\"prediction\"]))\n",
    "\n",
    "predictions_test_cache = predictions_test.cache()\n",
    "predictions_test_cache.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12dc4ddf-a460-47f4-aa05-aa4676a50635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "268387"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_cache.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e204159-ef8b-4eeb-b39d-13b83822f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1863549184105326\n"
     ]
    }
   ],
   "source": [
    "rmse_content_test = evaluator.evaluate(predictions_test_cache)\n",
    "print(rmse_content_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbda2e6-1445-44bd-ae9e-6efcf96575cd",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "At the end of this project, the content based recommender provided marginal improvement over the global baseline.  The RMSE for the full training data for the global recommender was 1.21.  The RMSE for testing data global recommender was 1.23.  The previous assignment had a content based recommender with a training RMSE of 1.18 and a testing RMSE of 1.34.  In comparison, the subsetted data in Google Colab resulted in a content based recommender with a test RMSE of 1.21 and a testing RMSE of 1.24.  The testing RMSE showed a 6% improvement over the previous project’s content based recommender.  With the full user-ratings dataset and the top 100 anime, the training and testing RMSE were both 1.19.  The testing RMSE showed an 11% improvement.  Overall, the content based recommender for the 100 top animes did have the best RMSE test score, but only by hundredths of a rating point.  The lack of diversity from using only 100 anime is likely not worth the incredibly marginal improvement.  \n",
    "\n",
    "The next steps for improving past the global baseline would be to tune the parameters on the content based recommender.  The top 100 anime recommender used 5000 features - potentially increasing this further would help adjust cosine similarity between anime.  The full anime dataset could be used.  Both of these might necessitate more RAM than 8GiB.  Regarding security, I would also like to figure out how to remove the master node IP from the Spark configuration.  Given that I had attempted an SSH tunnel to prevent exposure of the IP, these efforts were not really successful if it was exposed in the Spark configuration in Jupyter notebook.  \n",
    "\n",
    "From this project, I found that PySpark (in both Google Colab and AWS) shows incredible utility for scaling data operations as long as it is used properly.  Using too many workers while failing to properly partition data, failing to cache or persist dataframes, and failing to unpersist dataframes can result in poor performance and long runtimes.  As I learned more about PySpark, it did allow me to perform data transformations on a scale that would not be feasible with Pandas.  My next steps for furthering my experience with PySpark would be to develop a better understanding of how to partition data evenly, how to prevent mass shuffling operations between executors/partitions, and how to debug issues by using the tasks in the Spark UI.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84671593-41d4-43c2-bf1a-f77a7763b623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
